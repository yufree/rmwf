---
title: "Statistical Analysis"
author: "Miao Yu"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval = F)
```

## Demo data

We prepared 5 NIST 1950 samples and 5 matrix blank samples as demo data.

```{r}
library(rmwf)
data("mzrt")
```

## Bottom-up analysis

Bottom-up analysis mean the model for each metabolite. In this case, we could find out which metabolite will be affected by our experiment design. However, take care of multiple comparison issue.

$$
metabolite = f(control/treatment, co-variables)
$$

## Top-down analysis

Top-down analysis mean the model for output. In this case, we could evaluate the contribution of each metabolites. You need variable selection to make a better model.

$$
control/treatment = f(metabolite 1,metabolite 2,...,metaboliteN,co-varuables)
$$

### Linear regression

Here we use limma package to perform differental analysis.

```{r}
library(limma)
design <- model.matrix(~factor(mzrt$group$sample_group))
fit <- lmFit(log2(mzrt$data+1), design)
fit2  <- eBayes(fit)
limma::volcanoplot(fit2,coef = 2)
idx <- topTable(fit2,coef=2,adjust.method = "bonferroni",sort.by = 'none',number = nrow(fit2))
fs <- enviGCMS::getfilter(mzrt,idx$adj.P.Val<0.05)
sum(idx$P.Value<0.05)
sum(idx$adj.P.Val<0.05)
```

### Variable selection

By changing the machine learning method of the following code, all the algorithms (238 in total) supported by caret package can be used to find biomarker.

```{r}
library(caret)
## Splitting data
trainIndex <- createDataPartition(fs$group$sample_group, p = .5, 
                                  list = FALSE, 
                                  times = 1)
## Get the training and testing datasets
train <- fs$data[, trainIndex]
train <- cbind.data.frame(Y=fs$group$sample_group[trainIndex],t(train))
test  <- fs$data[,-trainIndex]
test  <- cbind.data.frame(Y=fs$group$sample_group[-trainIndex],t(test))
## variable selection by elastic net
lambad.grid <- 10^seq(2,-2,length=100)
alpha.grid <- seq(0,1,length=10)
trnCtrl=trainControl(method = 'repeatedCV',number = 10,repeats = 5)
srchGrd = expand.grid(.alpha=alpha.grid,.lambda=lambad.grid)
mytrain <- train(Y~.,data=train,
                 method="glmnet",
                 tuneGrid=srchGrd,
                 trControl=trnCtrl,
                 standardize=T,maxit=1000000)
plot(mytrain)
mytrain$bestTune
coef(mytrain)
pROC::roc(test$Y, as.numeric(predict(mytrain, test[,-1], type = "response")))

## Variable impoartance
library(gWQS)
result <- gwqs(Y~wqs,data = train,mix_name = colnames(train)[-1],q = 4,plots = T,pred = 0.3)

```

### compare multiple models

Here is the code to compare multiple models

```{r sa}
library(caret)
## Spliting data
trainIndex <- createDataPartition(fs$group$sample_group, p = .5, 
                                  list = FALSE, 
                                  times = 1)
## Get the training and testing datasets
train <- fs$data[, trainIndex]
train <- cbind.data.frame(Y=fs$group$sample_group[trainIndex],t(train))
test  <- fs$data[,-trainIndex]
test  <- cbind.data.frame(Y=fs$group$sample_group[-trainIndex],t(test))

## Train the model
library(doParallel)
registerDoParallel(4)
getDoParWorkers()
set.seed(123)
my_control <- trainControl(method = "cv", # for “cross-validation”
                           number = 5, # number of k-folds
                           savePredictions = "final",
                           classProbs=TRUE,
                           allowParallel = TRUE)
# test multiple model
library(caretEnsemble)
modellist <- caretList(Y~.,
                        data=train,
                        trControl = my_control,
                        methodList = c("lda",'rf'),
                        tuneList = NULL,
                        continue_on_fail = FALSE)
modellist
results <- resamples(modellist)
summary(results)

model_preds <- lapply(modellist, predict, newdata=test, type="prob")
model_preds <- lapply(model_preds, function(x) x[,"NIST1950"])
model_preds <- data.frame(model_preds)
caTools::colAUC(model_preds, test$Y)
```

### ensemble models

You can build a meta-model by ensemble multiple machine learning models as one.

```{r ensemble,eval=F}
# ensemble model
greedy_ensemble <- caretEnsemble(
  modellist, 
  metric="kappa",
  trControl=trainControl(
    number=2,
    summaryFunction=twoClassSummary,
    classProbs=TRUE
    ))
summary(greedy_ensemble)
model_preds$ensemble <- predict(greedy_ensemble, newdata=test, type="prob")
varImp(greedy_ensemble)
caTools::colAUC(model_preds, test$Y)

# glm stack model
glm_ensemble <- caretStack(
  modellist,
  method="glm",
  metric="ROC",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=twoClassSummary
  )
)
model_preds2 <- model_preds
model_preds2$ensemble <- predict(glm_ensemble, newdata=test, type="prob")
CF <- coef(glm_ensemble$ens_model$finalModel)[-1]
caTools::colAUC(model_preds2, test$Y)
```

## AutoML

AutoML (Automatic Machine Learning) made it easy for non-experts to experiment with machine learning. It can automate the process of training a large selection of candidate models. If you can refine your metabolomics as a $y = f(x)$ format, you can try AutoML to build a fine machine learning for prediction purpose. Here we use H2O/tinymodel as the AutoML platform.

```{r h2o}
path <- system.file("demodata/untarget", package = "rmwf")
files <- list.files(path,recursive = T,full.names = T)

# demo data from MTBLS28 https://www.ebi.ac.uk/metabolights/MTBLS28
MTBLS28 <- enviGCMS::getmzrtcsv(files[grepl('MTBLS28pos',files)])

lv <- ifelse(grepl('Control',MTBLS28$group$sample_group),'control','case')
trainIndex <- sample(1:1005,700)
train <- MTBLS28$data[, trainIndex]
train <- cbind.data.frame(Y=lv[trainIndex],t(train))
test  <- MTBLS28$data[,-trainIndex]
test  <- cbind.data.frame(Y=lv[-trainIndex],t(test))

y = 'Y'
pred = setdiff(names(train), y)
#convert variables to factors
train[,y] = as.factor(train[,y])
test[,y] = as.factor(test[,y])

library('h2o')
invisible(h2o.init())
train_h = as.h2o(train)
test_h = as.h2o(test)
# Run AutoML for 20 base models
aml = h2o.automl(x = pred, y = y,
                  training_frame = train_h,
                  max_models = 20,
                  seed = 42
                 )
# AutoML Leaderboard
lb <- h2o.get_leaderboard(object = aml, extra_columns = "ALL")
lb
# prediction result on test data
prediction = h2o.predict(aml@leader, test_h[,-1])
# create a confusion matrix
caret::confusionMatrix(test$Y, as.data.frame(prediction)$predict)
# explain model
h2o.explain(aml)
## no vip for stacked model
m <- h2o.get_best_model(aml)
m <- h2o.getModel("GBM_1_AutoML_2_20220721_115832")
h2o.varimp(m)
# close h2o connection
h2o.shutdown(prompt = F)
```

Here is the python version:

```{python}
import h2o
from h2o.automl import H2OAutoML

# Start the H2O cluster (locally)
h2o.init()

# Import a sample binary outcome train/test set into H2O
train = h2o.import_file("train.csv")
test = h2o.import_file("test.csv")

# Identify predictors and response
x = train.columns
y = "Y"
x.remove(y)
# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

# Run AutoML for 20 base models
aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)

# View the AutoML Leaderboard
lb = aml.leaderboard
lb.head(rows=lb.nrows)  
aml.leader
preds = aml.predict(test)
exa = aml.explain(test)
va_plot = aml.varimp_heatmap()
```

Here is the autogluon version:

```{r}
reticulate::use_condaenv('autogluon')
```

```{python}
from autogluon.tabular import TabularDataset, TabularPredictor
train_data = TabularDataset('train.csv')
label = 'Y'
save_path = 'agModels-predictClass'
predictor = TabularPredictor(label=label, path=save_path).fit(train_data)
test_data = TabularDataset('test.csv')
y_test = test_data[label]
test_data_nolab = test_data.drop(columns=[label])
predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file

y_pred = predictor.predict(test_data_nolab)
print("Predictions:  \\n", y_pred)
perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)
predictor.leaderboard(test_data, silent=True)

time_limit = 3600  # for quick demonstration only, you should set this to longest time you are willing to wait (in seconds)
metric = 'roc_auc'  # specify your evaluation metric here
predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')
predictor.leaderboard(test_data, silent=True)
predictor.feature_importance(test_data)

# rule based
predictor2 = TabularPredictor(label='Y')
predictor2.fit(train_data, presets='interpretable')
predictor2.leaderboard()

```

